2024-05-19 19:02:49,666 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:02:49,803 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:02:49,969 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:05:35,126 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:05:35,251 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:05:35,463 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:05:35,686 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:05:35,752 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:05:35,753 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:05:35,763 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:05:35,763 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:15:03,540 - INFO - joeynmt.prediction - Generation took 567.7376[sec]. (No references given)
2024-05-19 19:22:45,163 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:22:45,280 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:22:45,414 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:22:45,527 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:22:45,555 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:22:45,555 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:22:45,563 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:22:45,563 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:32:23,749 - INFO - joeynmt.prediction - Generation took 578.1423[sec]. (No references given)
2024-05-19 19:32:28,685 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:32:28,797 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:32:28,920 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:32:29,038 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:32:29,062 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:32:29,062 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:32:29,070 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:32:29,070 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:35:08,158 - INFO - joeynmt.prediction - Generation took 159.0512[sec]. (No references given)
2024-05-19 19:35:12,975 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:35:13,081 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:35:13,211 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:35:13,326 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:35:13,345 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:35:13,345 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:35:13,351 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:35:13,351 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:39:15,218 - INFO - joeynmt.prediction - Generation took 241.7980[sec]. (No references given)
2024-05-19 19:39:20,669 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:39:20,789 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:39:20,936 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:39:21,057 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:39:21,081 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:39:21,081 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:39:21,088 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:39:21,089 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:45:41,379 - INFO - joeynmt.prediction - Generation took 380.2112[sec]. (No references given)
2024-05-19 19:45:47,368 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:45:47,482 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:45:47,618 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:45:47,728 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:45:47,749 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:45:47,749 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:45:47,759 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:45:47,759 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:15:28,147 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:15:28,393 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:15:28,742 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:15:28,912 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:15:28,941 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:15:28,941 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:15:28,952 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:15:28,952 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:24:31,978 - INFO - joeynmt.prediction - Generation took 542.9777[sec]. (No references given)
2024-05-19 20:24:37,230 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:24:37,333 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:24:37,459 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:24:37,559 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:24:37,581 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:24:37,582 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:24:37,590 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:24:37,590 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:27:05,091 - INFO - joeynmt.prediction - Generation took 147.4669[sec]. (No references given)
2024-05-19 20:27:09,553 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:27:09,655 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:27:09,775 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:27:09,883 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:27:09,902 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:27:09,902 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:27:09,917 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:27:09,917 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:31:01,950 - INFO - joeynmt.prediction - Generation took 231.9971[sec]. (No references given)
2024-05-19 20:31:06,463 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:31:06,565 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:31:06,705 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:31:06,819 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:31:06,841 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:31:06,841 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:31:06,850 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:31:06,851 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:36:07,770 - INFO - joeynmt.prediction - Generation took 300.8853[sec]. (No references given)
2024-05-19 20:36:12,250 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:36:12,364 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:36:12,505 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:36:12,623 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:36:12,643 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:36:12,643 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:36:12,650 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:36:12,651 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:43:39,227 - INFO - joeynmt.prediction - Generation took 446.5398[sec]. (No references given)
2024-05-19 20:43:43,853 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:43:43,966 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:43:44,086 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:43:44,204 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:43:44,224 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:43:44,224 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:43:44,231 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:43:44,231 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:53:30,363 - INFO - joeynmt.prediction - Generation took 586.0982[sec]. (No references given)
2024-05-19 20:53:34,837 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:53:34,943 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:53:35,064 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:53:35,173 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:53:35,195 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:53:35,195 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:53:35,209 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:53:35,209 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=7, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 21:06:16,083 - INFO - joeynmt.prediction - Generation took 760.8417[sec]. (No references given)
2024-05-19 21:06:20,587 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 21:06:20,693 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 21:06:20,818 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 21:06:20,938 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 21:06:20,959 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:06:20,959 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:06:20,974 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 21:06:20,974 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 21:21:53,144 - INFO - joeynmt.prediction - Generation took 932.1367[sec]. (No references given)
2024-05-19 21:21:57,551 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 21:21:57,656 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 21:21:57,772 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 21:21:57,875 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 21:21:57,896 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:21:57,896 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:21:57,903 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 21:21:57,903 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=9, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 21:40:05,240 - INFO - joeynmt.prediction - Generation took 1087.3028[sec]. (No references given)
2024-05-19 21:40:09,700 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 21:40:09,806 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 21:40:09,929 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 21:40:10,036 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 21:40:10,055 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:40:10,055 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:40:10,070 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 21:40:10,070 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=10, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 22:00:49,466 - INFO - joeynmt.prediction - Generation took 1239.3619[sec]. (No references given)
