2024-05-19 19:02:49,666 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:02:49,803 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:02:49,969 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:05:35,126 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:05:35,251 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:05:35,463 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:05:35,686 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:05:35,752 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:05:35,753 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:05:35,763 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:05:35,763 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:15:03,540 - INFO - joeynmt.prediction - Generation took 567.7376[sec]. (No references given)
2024-05-19 19:22:45,163 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:22:45,280 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:22:45,414 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:22:45,527 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:22:45,555 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:22:45,555 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:22:45,563 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:22:45,563 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:32:23,749 - INFO - joeynmt.prediction - Generation took 578.1423[sec]. (No references given)
2024-05-19 19:32:28,685 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:32:28,797 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:32:28,920 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:32:29,038 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:32:29,062 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:32:29,062 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:32:29,070 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:32:29,070 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:35:08,158 - INFO - joeynmt.prediction - Generation took 159.0512[sec]. (No references given)
2024-05-19 19:35:12,975 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:35:13,081 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:35:13,211 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:35:13,326 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:35:13,345 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:35:13,345 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:35:13,351 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:35:13,351 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:39:15,218 - INFO - joeynmt.prediction - Generation took 241.7980[sec]. (No references given)
2024-05-19 19:39:20,669 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:39:20,789 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:39:20,936 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:39:21,057 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:39:21,081 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:39:21,081 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:39:21,088 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:39:21,089 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 19:45:41,379 - INFO - joeynmt.prediction - Generation took 380.2112[sec]. (No references given)
2024-05-19 19:45:47,368 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 19:45:47,482 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 19:45:47,618 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 19:45:47,728 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 19:45:47,749 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:45:47,749 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 19:45:47,759 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 19:45:47,759 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:15:28,147 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:15:28,393 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:15:28,742 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:15:28,912 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:15:28,941 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:15:28,941 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:15:28,952 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:15:28,952 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:24:31,978 - INFO - joeynmt.prediction - Generation took 542.9777[sec]. (No references given)
2024-05-19 20:24:37,230 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:24:37,333 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:24:37,459 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:24:37,559 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:24:37,581 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:24:37,582 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:24:37,590 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:24:37,590 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:27:05,091 - INFO - joeynmt.prediction - Generation took 147.4669[sec]. (No references given)
2024-05-19 20:27:09,553 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:27:09,655 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:27:09,775 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:27:09,883 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:27:09,902 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:27:09,902 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:27:09,917 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:27:09,917 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:31:01,950 - INFO - joeynmt.prediction - Generation took 231.9971[sec]. (No references given)
2024-05-19 20:31:06,463 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:31:06,565 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:31:06,705 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:31:06,819 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:31:06,841 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:31:06,841 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:31:06,850 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:31:06,851 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:36:07,770 - INFO - joeynmt.prediction - Generation took 300.8853[sec]. (No references given)
2024-05-19 20:36:12,250 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:36:12,364 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:36:12,505 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:36:12,623 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:36:12,643 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:36:12,643 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:36:12,650 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:36:12,651 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:43:39,227 - INFO - joeynmt.prediction - Generation took 446.5398[sec]. (No references given)
2024-05-19 20:43:43,853 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:43:43,966 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:43:44,086 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:43:44,204 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:43:44,224 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:43:44,224 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:43:44,231 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:43:44,231 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 20:53:30,363 - INFO - joeynmt.prediction - Generation took 586.0982[sec]. (No references given)
2024-05-19 20:53:34,837 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 20:53:34,943 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 20:53:35,064 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 20:53:35,173 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 20:53:35,195 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:53:35,195 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 20:53:35,209 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 20:53:35,209 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=7, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 21:06:16,083 - INFO - joeynmt.prediction - Generation took 760.8417[sec]. (No references given)
2024-05-19 21:06:20,587 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 21:06:20,693 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 21:06:20,818 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 21:06:20,938 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 21:06:20,959 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:06:20,959 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:06:20,974 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 21:06:20,974 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 21:21:53,144 - INFO - joeynmt.prediction - Generation took 932.1367[sec]. (No references given)
2024-05-19 21:21:57,551 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 21:21:57,656 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 21:21:57,772 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 21:21:57,875 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 21:21:57,896 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:21:57,896 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:21:57,903 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 21:21:57,903 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=9, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 21:40:05,240 - INFO - joeynmt.prediction - Generation took 1087.3028[sec]. (No references given)
2024-05-19 21:40:09,700 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-19 21:40:09,806 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-19 21:40:09,929 - INFO - joeynmt.model - Enc-dec model built.
2024-05-19 21:40:10,036 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-19 21:40:10,055 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:40:10,055 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-19 21:40:10,070 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-19 21:40:10,070 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=10, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-19 22:00:49,466 - INFO - joeynmt.prediction - Generation took 1239.3619[sec]. (No references given)
2024-05-20 18:37:18,259 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 18:37:18,390 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 18:37:18,588 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 18:37:18,743 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 18:37:18,780 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:37:18,781 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:37:18,822 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 18:37:18,822 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 18:45:33,232 - INFO - joeynmt.prediction - Generation took 494.3702[sec]. (No references given)
2024-05-20 18:45:37,510 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 18:45:37,597 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 18:45:37,695 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 18:45:37,792 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 18:45:37,808 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:45:37,808 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:45:37,812 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 18:45:37,813 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 18:47:48,851 - INFO - joeynmt.prediction - Generation took 131.0037[sec]. (No references given)
2024-05-20 18:47:52,757 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 18:47:52,844 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 18:47:52,936 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 18:47:53,024 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 18:47:53,045 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:47:53,045 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:47:53,050 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 18:47:53,050 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 18:51:16,143 - INFO - joeynmt.prediction - Generation took 203.0620[sec]. (No references given)
2024-05-20 18:51:20,091 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 18:51:20,175 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 18:51:20,271 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 18:51:20,369 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 18:51:20,386 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:51:20,387 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:51:20,391 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 18:51:20,392 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 18:56:09,954 - INFO - joeynmt.prediction - Generation took 289.5298[sec]. (No references given)
2024-05-20 18:56:13,973 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 18:56:14,065 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 18:56:14,168 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 18:56:14,264 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 18:56:14,283 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:56:14,283 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 18:56:14,288 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 18:56:14,288 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 19:03:16,157 - INFO - joeynmt.prediction - Generation took 421.8322[sec]. (No references given)
2024-05-20 19:03:20,386 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 19:03:20,478 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 19:03:20,580 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 19:03:20,679 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 19:03:20,697 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:03:20,697 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:03:20,702 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 19:03:20,702 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 19:13:23,969 - INFO - joeynmt.prediction - Generation took 603.2317[sec]. (No references given)
2024-05-20 19:13:28,260 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 19:13:28,366 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 19:13:28,484 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 19:13:28,590 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 19:13:28,609 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:13:28,609 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:13:28,616 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 19:13:28,616 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=7, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 19:28:17,407 - INFO - joeynmt.prediction - Generation took 888.7541[sec]. (No references given)
2024-05-20 19:28:22,265 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 19:28:22,354 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 19:28:22,454 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 19:28:22,553 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 19:28:22,573 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:28:22,574 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:28:22,579 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 19:28:22,580 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 19:45:44,034 - INFO - joeynmt.prediction - Generation took 1041.4196[sec]. (No references given)
2024-05-20 19:45:48,570 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 19:45:48,667 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 19:45:48,780 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 19:45:48,881 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 19:45:48,899 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:45:48,899 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 19:45:48,904 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 19:45:48,904 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=9, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 20:02:32,101 - INFO - joeynmt.prediction - Generation took 1003.1655[sec]. (No references given)
2024-05-20 20:02:36,082 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2024-05-20 20:02:36,171 - INFO - joeynmt.model - Building an encoder-decoder model...
2024-05-20 20:02:36,277 - INFO - joeynmt.model - Enc-dec model built.
2024-05-20 20:02:36,375 - INFO - joeynmt.helpers - Load model from /mnt/c/Users/Yuliia/Desktop/Y3S6/machine_translation/mt-2024-ex05/mt-exercise-5/models/transformer_bpe2k/best.ckpt.
2024-05-20 20:02:36,392 - INFO - joeynmt.tokenizers - nl tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 20:02:36,392 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2024-05-20 20:02:36,397 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2024-05-20 20:02:36,397 - INFO - joeynmt.prediction - Predicting 1777 example(s)... (Beam search with beam_size=10, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2024-05-20 20:21:28,820 - INFO - joeynmt.prediction - Generation took 1132.3923[sec]. (No references given)
